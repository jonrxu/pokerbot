# Exploitative Deep CFR Training Guide

**Train an exploitative poker bot using the SAME Deep CFR method as your GTO model.**

---

## üéØ What Is This?

This trains a Deep CFR agent to **exploit** your GTO model using the exact same algorithm (Deep CFR), just with a different training setup:

| Training Type | Opponent | Result |
|---------------|----------|--------|
| **GTO (Nash)** | Self-play (both players update) | Nash equilibrium (unexploitable) |
| **Exploitative (Best Response)** | Fixed GTO opponent (only we update) | Best response (exploits GTO) |

**No LLMs, no text prompts, no Qwen** - just pure Deep CFR training!

---

## üß† How It Works

### Regular GTO Training (What you already have)
```
Player 0 (CFR) vs Player 1 (CFR)
     ‚Üì                  ‚Üì
Both update strategies
     ‚Üì
Converges to Nash Equilibrium
(Both play optimally, unexploitable)
```

### Exploitative Training (What we're adding)
```
Player 0 (CFR - Training) vs Player 1 (GTO - Fixed)
     ‚Üì                           ‚Üì
Only Player 0 updates      Never updates (frozen)
     ‚Üì
Learns Best Response
(Exploits GTO's predictable patterns)
```

---

## üöÄ Quick Start

### Step 1: Setup Modal
```bash
pip install modal
modal setup
```

### Step 2: Upload GTO Checkpoint
```bash
# Upload your trained GTO checkpoint
modal volume put poker-bot-checkpoints \
  checkpoints/checkpoint_1000.pt \
  checkpoint_1000.pt
```

### Step 3: Train Exploitative Agent
```bash
# Train for 1000 iterations (same as GTO training)
modal run modal_deploy/train_exploitative_cfr.py::main \
  --gto-checkpoint checkpoint_1000.pt \
  --num-iterations 1000 \
  --trajectories-per-iteration 1000
```

**That's it!** Uses the exact same Deep CFR algorithm as your GTO model.

---

## üìä Expected Results

After **1000 iterations** (same as GTO training):

| Metric | Target | Meaning |
|--------|--------|---------|
| **Avg Payoff** | +100 to +300 chips/hand | Exploiting GTO |
| **Win Rate** | 55-65% | Beating GTO consistently |
| **Exploitability** | High (doesn't matter!) | Not trying to be unexploitable |

**Key difference:**
- GTO aims for ~0 exploitability (Nash equilibrium)
- Exploitative aims for maximum profit vs GTO (doesn't care about exploitability)

---

## üéÆ Training Details

### What Happens During Training

```python
for iteration in range(1000):
    # 1. Generate trajectories
    for _ in range(1000):
        # Play hand: Exploitative (us) vs GTO (opponent)
        # - GTO uses fixed average strategy (Nash)
        # - We use current strategy (updating)

    # 2. Update our agent only
    # - Compute counterfactual values
    # - Update regrets based on GTO's strategy
    # - Train value and policy networks
    # - GTO opponent never updates (frozen)

    # 3. Save checkpoint every 100 iterations
```

### Key Points

1. **Same algorithm** - Deep CFR with regret matching
2. **Same networks** - Value network + Policy network (ResNet)
3. **Same encoding** - 167-dimensional state features
4. **Different opponent** - Fixed GTO instead of self-play

---

## üí∞ Cost Estimate

**Modal GPU pricing (A10G):**
- ~$1.10/hour
- 1000 iterations ‚âà 4-6 hours
- **Total: ~$5-7**

Compare to:
- GTO training: ~$8-16 (longer, self-play)
- LLM version: ~$25-35 (Tinker + Modal)

**Much simpler and cheaper than LLM approach!**

---

## üì• Download Trained Model

```bash
# List checkpoints
modal volume ls poker-exploitative-cfr-checkpoints

# Download final checkpoint
modal volume get poker-exploitative-cfr-checkpoints \
  exploitative_cfr_final.pt \
  ./exploitative_model.pt

# Or download specific iteration
modal volume get poker-exploitative-cfr-checkpoints \
  exploitative_cfr_iter_1000.pt \
  ./exploitative_iter_1000.pt
```

---

## üé≤ Using Your Exploitative Model

```python
import torch
from poker_game.game import PokerGame
from poker_game.state_encoder import StateEncoder
from poker_game.information_set import get_information_set
from models.value_policy_net import ValuePolicyNet
from training.deep_cfr import DeepCFR

# Load exploitative model
checkpoint = torch.load('exploitative_model.pt')

# Initialize networks
game = PokerGame()
encoder = StateEncoder()
value_net = ValuePolicyNet(input_dim=encoder.feature_dim)
policy_net = ValuePolicyNet(input_dim=encoder.feature_dim)

# Load weights
value_net.load_state_dict(checkpoint['value_net_state'])
policy_net.load_state_dict(checkpoint['policy_net_state'])

# Create agent
exploitative_agent = DeepCFR(
    value_net=value_net,
    policy_net=policy_net,
    state_encoder=encoder,
    game=game,
)

# Load memories
exploitative_agent.regret_memory = checkpoint['regret_memory']
exploitative_agent.strategy_memory = checkpoint['strategy_memory']

# Use it to play
state = game.reset()
legal_actions = game.get_legal_actions(state)
info_set = get_information_set(state, 0)
strategy = exploitative_agent.get_average_strategy(info_set, legal_actions)

# strategy is a dict: {(Action, amount): probability}
```

---

## üî¨ Why This Works

### Game Theory Explanation

**Nash Equilibrium (GTO):**
- Both players play optimally
- Neither can improve by changing strategy
- Exploitability ‚âà 0 (unexploitable)
- **BUT:** Against non-GTO opponents, it's not maximally profitable

**Best Response (Exploitative):**
- One player exploits the other's fixed strategy
- Can be counter-exploited by adaptive opponents
- Exploitability > 0 (exploitable if opponent adapts)
- **BUT:** Against fixed GTO, it's maximally profitable

**Your use case:**
Since GTO is **fixed** (doesn't adapt), best response is optimal!

---

## üìà Monitoring Training

Training outputs metrics every iteration:

```
================================================================================
Iteration 500/1000
================================================================================
Avg Payoff: +187.45 chips
Win Rate: 58.30%
Trajectories: 1000
Value Buffer: 4532
Policy Buffer: 4532
Saving checkpoint to /checkpoints/exploitative_cfr_iter_0500.pt
  ‚≠ê New best avg payoff: +187.45
```

**What to watch:**
- **Avg Payoff** - Should increase over time (target: +100 to +300)
- **Win Rate** - Should be >55% (exploiting GTO)
- **Stable values** - Should converge after ~500-1000 iterations

---

## üéõÔ∏è Configuration Options

### Basic Usage
```bash
modal run modal_deploy/train_exploitative_cfr.py::main \
  --gto-checkpoint checkpoint_1000.pt \
  --num-iterations 1000
```

### All Options
```bash
modal run modal_deploy/train_exploitative_cfr.py::main \
  --gto-checkpoint checkpoint_1000.pt \
  --num-iterations 1000 \
  --trajectories-per-iteration 1000
```

**Parameters:**

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--gto-checkpoint` | Required | Your GTO checkpoint name |
| `--num-iterations` | 1000 | Number of training iterations |
| `--trajectories-per-iteration` | 1000 | Hands per iteration |

---

## üÜö Comparison: All Approaches

| Approach | Method | Cost | Complexity | vs GTO Performance |
|----------|--------|------|------------|-------------------|
| **Exploitative CFR** ‚≠ê | Deep CFR | ~$5-7 | Low (same as GTO) | +100 to +300 chips/hand |
| **Exploitative LLM (Modal)** | Policy Gradient | ~$2-3 | Medium | +50 to +150 chips/hand |
| **Exploitative LLM (Tinker)** | LoRA RL | ~$25-35 | High | +50 to +150 chips/hand |

**Recommendation: Use Exploitative CFR** ‚úÖ
- Same proven algorithm as GTO
- Better performance
- Lower cost than Tinker
- No new dependencies

---

## üß™ Experiment: Compare Strategies

Train multiple exploitative agents with different GTO checkpoints:

```bash
# Exploit early GTO (iteration 100)
modal run modal_deploy/train_exploitative_cfr.py::main \
  --gto-checkpoint checkpoint_0100.pt

# Exploit mid GTO (iteration 500)
modal run modal_deploy/train_exploitative_cfr.py::main \
  --gto-checkpoint checkpoint_0500.pt

# Exploit final GTO (iteration 1000)
modal run modal_deploy/train_exploitative_cfr.py::main \
  --gto-checkpoint checkpoint_1000.pt
```

**Hypothesis:** Weaker GTO should be easier to exploit (higher payoff).

---

## üîß Troubleshooting

### Issue: "GTO checkpoint not found"

```bash
# List what's in your volume
modal volume ls poker-bot-checkpoints

# Upload missing checkpoint
modal volume put poker-bot-checkpoints \
  checkpoints/checkpoint_1000.pt \
  checkpoint_1000.pt
```

### Issue: Avg payoff not increasing

**Possible causes:**
1. GTO checkpoint too weak - not enough training iterations
2. Learning rate too low - try increasing
3. Not enough trajectories - increase `--trajectories-per-iteration`

**Solutions:**
```bash
# More trajectories per iteration
modal run ... --trajectories-per-iteration 2000

# More total training
modal run ... --num-iterations 2000
```

### Issue: Win rate < 50%

**This means your exploitative agent is losing to GTO!**

**Possible causes:**
1. Bug in training code
2. GTO checkpoint corrupted
3. Not enough training

**Debug:**
```python
# Check that GTO checkpoint loads correctly
import torch
checkpoint = torch.load('checkpoint_1000.pt')
print(checkpoint.keys())  # Should have 'value_net_state', 'policy_net_state', etc.
```

---

## üìö Theory: Best Response vs Nash

### Nash Equilibrium (GTO)
```
max min_opponent payoff
(Maximize worst-case payoff)
```
- Safe strategy
- Unexploitable
- But not maximally profitable vs non-GTO

### Best Response (Exploitative)
```
max payoff vs fixed_opponent
(Maximize payoff vs this specific opponent)
```
- Risky if opponent adapts
- Exploitable
- But maximally profitable vs fixed opponent

**Your case:** GTO is fixed ‚Üí Best Response is optimal! ‚úÖ

---

## üéØ Expected Exploitation

After training, your exploitative agent will:

1. **Over-bluff** where GTO folds too often
2. **Over-fold** where GTO bluffs too little
3. **Exploit bet sizing** - GTO uses predictable sizes
4. **Exploit position** - GTO plays same in/out of position

Example:
- GTO folds 33% on river bluffs (equilibrium)
- Exploitative bluffs 50% (over-bluffs to exploit folds)
- Profit: +EV because GTO can't adapt (it's frozen)

---

## üöÄ Next Steps

1. **Train your GTO model** if you haven't:
   ```bash
   modal run modal_deploy/train.py
   ```

2. **Train exploitative model**:
   ```bash
   modal run modal_deploy/train_exploitative_cfr.py::main \
     --gto-checkpoint checkpoint_1000.pt
   ```

3. **Download both models**:
   ```bash
   modal volume get poker-bot-checkpoints \
     checkpoint_1000.pt ./gto_model.pt

   modal volume get poker-exploitative-cfr-checkpoints \
     exploitative_cfr_final.pt ./exploitative_model.pt
   ```

4. **Evaluate head-to-head** - Your exploitative model should win!

---

## üí° Advanced: Iterative Exploitation

Train a sequence of exploitative models:

```
Round 1: GTO vs Exploitative_1 ‚Üí Exploitative_1 wins
Round 2: Exploitative_1 vs Exploitative_2 ‚Üí Exploitative_2 wins
Round 3: Exploitative_2 vs GTO ‚Üí GTO wins!
(Rock-paper-scissors)
```

This demonstrates the non-transitivity of poker strategies!

---

## üìñ Summary

**What you get:**
- ‚úÖ Deep CFR exploitative agent (same algorithm as GTO)
- ‚úÖ Trained to exploit your GTO model
- ‚úÖ ~$5-7 cost for full training
- ‚úÖ 55-65% win rate vs GTO
- ‚úÖ +100 to +300 chips/hand expected value

**What you DON'T need:**
- ‚ùå LLMs, Qwen, or text prompts
- ‚ùå Tinker API or external services
- ‚ùå New algorithms or architectures

**Just Deep CFR with a different training setup!** üéâ

---

Questions? The code is in:
- `training/exploitative_trainer.py` - Exploitative Deep CFR trainer
- `modal_deploy/train_exploitative_cfr.py` - Modal deployment
