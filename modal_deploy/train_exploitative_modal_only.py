"""Modal-only exploitative LLM training (no Tinker required).

This script trains an exploitative poker LLM entirely on Modal using:
- HuggingFace transformers for the LLM
- PyTorch for training
- Policy gradient (REINFORCE) for RL
- Your Deep CFR GTO checkpoint as opponent

No external APIs required!
"""

import modal
import os

# Get the project root directory
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# Modal image with all dependencies
image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install(
        "torch>=2.0.0",
        "transformers>=4.36.0",
        "accelerate>=0.25.0",
        "numpy>=1.24.0",
        "tqdm>=4.65.0",
    )
    .add_local_dir(
        os.path.join(project_root, "poker_game"),
        "/root/poker_game"
    )
    .add_local_dir(
        os.path.join(project_root, "models"),
        "/root/models"
    )
    .add_local_dir(
        os.path.join(project_root, "training"),
        "/root/training"
    )
    .add_local_dir(
        os.path.join(project_root, "llm_poker"),
        "/root/llm_poker"
    )
)

# Volumes
checkpoint_volume = modal.Volume.from_name(
    "poker-exploitative-modal-checkpoints",
    create_if_missing=True
)
gto_checkpoint_volume = modal.Volume.from_name(
    "poker-bot-checkpoints",
    create_if_missing=True
)

app = modal.App("poker-exploitative-modal-only")


@app.function(
    image=image,
    volumes={
        "/checkpoints": checkpoint_volume,
        "/gto_checkpoints": gto_checkpoint_volume,
    },
    gpu="T4",  # T4 GPU - good balance of cost and performance
    timeout=36000,  # 10 hours
    memory=16384,  # 16GB RAM
)
def train_exploitative_modal(
    model_name: str = "Qwen/Qwen2.5-3B-Instruct",
    gto_checkpoint_name: str = "checkpoint_1000.pt",
    num_epochs: int = 20,
    episodes_per_epoch: int = 32,
    learning_rate: float = 1e-5,

    # Reward bonuses
    bluff_bonus: float = 0.5,
    aggression_bonus: float = 0.1,
    fold_equity_bonus: float = 0.3,
    exploitative_sizing_bonus: float = 0.2,

    # Training parameters
    temperature: float = 0.7,
    gamma: float = 0.99,
):
    """Train exploitative LLM on Modal (no Tinker).

    This runs entirely on Modal with no external API calls.
    """
    import sys
    sys.path.insert(0, "/root")

    import torch
    from tqdm import tqdm
    from llm_poker.llm_agent import PokerLLMAgent
    from llm_poker.policy_gradient import PolicyGradientTrainer, Episode
    from llm_poker.exploitative_env import ExploitativePokerEnv

    print("=" * 80)
    print("EXPLOITATIVE LLM POKER TRAINING (Modal Only)")
    print("=" * 80)
    print(f"Model: {model_name}")
    print(f"GTO Checkpoint: {gto_checkpoint_name}")
    print(f"Epochs: {num_epochs}, Episodes/Epoch: {episodes_per_epoch}")
    print(f"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}")
    print("=" * 80)

    # Initialize LLM agent
    device = "cuda" if torch.cuda.is_available() else "cpu"
    agent = PokerLLMAgent(
        model_name=model_name,
        device=device,
        temperature=temperature,
    )

    # Initialize policy gradient trainer
    trainer = PolicyGradientTrainer(
        agent=agent,
        learning_rate=learning_rate,
        gamma=gamma,
    )

    # Initialize exploitative environment
    gto_checkpoint_path = f"/gto_checkpoints/{gto_checkpoint_name}"
    env = ExploitativePokerEnv(
        gto_checkpoint_path=gto_checkpoint_path,
        bluff_bonus=bluff_bonus,
        aggression_bonus=aggression_bonus,
        fold_equity_bonus=fold_equity_bonus,
        exploitative_sizing_bonus=exploitative_sizing_bonus,
    )

    print(f"\nInitialized environment vs GTO: {gto_checkpoint_path}")
    print(f"Reward bonuses: bluff={bluff_bonus}, aggression={aggression_bonus}, "
          f"fold_equity={fold_equity_bonus}, sizing={exploitative_sizing_bonus}\n")

    # Training loop
    best_avg_return = float('-inf')
    metrics_history = []

    for epoch in range(num_epochs):
        print(f"\n{'='*80}")
        print(f"EPOCH {epoch + 1}/{num_epochs}")
        print(f"{'='*80}")

        # Collect episodes
        episodes = []
        epoch_stats = {
            'wins': 0,
            'losses': 0,
            'total_payoff': 0.0,
            'bluff_bonuses': 0.0,
        }

        print(f"Collecting {episodes_per_epoch} episodes...")
        for ep_idx in tqdm(range(episodes_per_epoch)):
            episode = collect_episode(env, agent)
            episodes.append(episode)

            # Track stats
            if episode.total_reward > 0:
                epoch_stats['wins'] += 1
            elif episode.total_reward < 0:
                epoch_stats['losses'] += 1

        # Calculate episode metrics
        avg_return = sum(ep.total_reward for ep in episodes) / len(episodes)
        win_rate = epoch_stats['wins'] / episodes_per_epoch

        print(f"\nEpisode Stats:")
        print(f"  Avg Return: {avg_return:.4f}")
        print(f"  Win Rate: {win_rate:.2%}")
        print(f"  Wins: {epoch_stats['wins']}, Losses: {epoch_stats['losses']}")

        # Training step
        print("\nTraining on collected episodes...")
        train_metrics = trainer.train_step(episodes)

        print(f"\nTraining Metrics:")
        print(f"  Loss: {train_metrics['loss']:.4f}")
        print(f"  Num Steps: {train_metrics['num_steps']}")

        # Save metrics
        epoch_metrics = {
            'epoch': epoch + 1,
            'avg_return': avg_return,
            'win_rate': win_rate,
            'loss': train_metrics['loss'],
            **epoch_stats,
        }
        metrics_history.append(epoch_metrics)

        # Save checkpoint every 5 epochs or if best performance
        if (epoch + 1) % 5 == 0 or avg_return > best_avg_return:
            checkpoint_path = f"/checkpoints/exploitative_epoch_{epoch+1:03d}.pt"
            print(f"\nSaving checkpoint to {checkpoint_path}")

            trainer.save_checkpoint(
                checkpoint_path,
                epoch=epoch + 1,
                metrics=epoch_metrics
            )

            # Also save the full model
            model_path = f"/checkpoints/exploitative_model_epoch_{epoch+1:03d}"
            agent.save(model_path)

            # Commit volume
            checkpoint_volume.commit()

            if avg_return > best_avg_return:
                best_avg_return = avg_return
                print(f"  â­ New best avg return: {best_avg_return:.4f}")

    # Save final model
    print("\n" + "="*80)
    print("TRAINING COMPLETE!")
    print("="*80)

    final_model_path = "/checkpoints/exploitative_model_final"
    agent.save(final_model_path)
    checkpoint_volume.commit()

    print(f"\nFinal model saved to: {final_model_path}")
    print(f"Best avg return: {best_avg_return:.4f}")

    # Print summary
    print("\n" + "="*80)
    print("TRAINING SUMMARY")
    print("="*80)
    for i, metrics in enumerate(metrics_history[-5:], start=max(0, len(metrics_history)-5)):
        print(f"Epoch {metrics['epoch']:3d}: "
              f"Return={metrics['avg_return']:+.4f}, "
              f"WinRate={metrics['win_rate']:.2%}, "
              f"Loss={metrics['loss']:.4f}")

    return {
        "status": "completed",
        "num_epochs": num_epochs,
        "best_avg_return": best_avg_return,
        "final_win_rate": metrics_history[-1]['win_rate'],
        "metrics_history": metrics_history,
    }


def collect_episode(env, agent) -> Episode:
    """Collect a single episode of play.

    Args:
        env: ExploitativePokerEnv instance
        agent: PokerLLMAgent instance

    Returns:
        Episode object with prompts, actions, rewards
    """
    import sys
    sys.path.insert(0, "/root")

    prompts = []
    actions = []
    rewards = []

    prompt = env.reset()
    done = False

    while not done:
        # Get action from LLM
        action_token = agent.get_action_token(prompt)

        # Store trajectory
        prompts.append(prompt)
        actions.append(action_token)

        # Step environment
        result = env.step(action_token)
        rewards.append(result.reward)

        done = result.done
        prompt = result.prompt

    # Calculate total reward
    total_reward = sum(rewards)

    return Episode(
        prompts=prompts,
        actions=actions,
        rewards=rewards,
        total_reward=total_reward,
    )


@app.local_entrypoint()
def main(
    model_name: str = "Qwen/Qwen2.5-3B-Instruct",
    gto_checkpoint: str = "checkpoint_1000.pt",
    num_epochs: int = 20,
    episodes_per_epoch: int = 32,
    learning_rate: float = 1e-5,
):
    """Local entrypoint to launch training on Modal."""
    print("\n" + "="*80)
    print("LAUNCHING EXPLOITATIVE LLM TRAINING ON MODAL")
    print("(No Tinker required - pure Modal + PyTorch)")
    print("="*80)
    print(f"\nConfiguration:")
    print(f"  Model: {model_name}")
    print(f"  GTO Checkpoint: {gto_checkpoint}")
    print(f"  Epochs: {num_epochs}")
    print(f"  Episodes/Epoch: {episodes_per_epoch}")
    print(f"  Learning Rate: {learning_rate}")
    print("\nStarting training...")

    result = train_exploitative_modal.remote(
        model_name=model_name,
        gto_checkpoint_name=gto_checkpoint,
        num_epochs=num_epochs,
        episodes_per_epoch=episodes_per_epoch,
        learning_rate=learning_rate,
    )

    print("\n" + "="*80)
    print("TRAINING RESULT")
    print("="*80)
    print(f"Status: {result['status']}")
    print(f"Best Avg Return: {result['best_avg_return']:.4f}")
    print(f"Final Win Rate: {result['final_win_rate']:.2%}")
    print(f"\nCheckpoints saved to Modal volume: poker-exploitative-modal-checkpoints")
    print("\nTo download your trained model:")
    print("  modal volume get poker-exploitative-modal-checkpoints \\")
    print("    exploitative_model_final \\")
    print("    ./exploitative_llm_model/")
