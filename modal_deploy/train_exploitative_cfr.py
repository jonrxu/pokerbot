"""Modal deployment for exploitative Deep CFR training.

This trains an exploitative poker bot using the SAME Deep CFR method
as the GTO model, but playing against a fixed GTO opponent instead of self-play.

This is called "best response" training - it learns to exploit the GTO player.
"""

import modal
import os

# Get the project root directory
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# Modal image with dependencies
image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install(
        "torch>=2.0.0",
        "numpy>=1.24.0",
        "tqdm>=4.65.0",
    )
    .add_local_dir(
        os.path.join(project_root, "poker_game"),
        "/root/poker_game",
    )
    .add_local_dir(
        os.path.join(project_root, "models"),
        "/root/models"
    )
    .add_local_dir(
        os.path.join(project_root, "training"),
        "/root/training"
    )
)

# Volumes
checkpoint_volume = modal.Volume.from_name(
    "poker-exploitative-cfr-checkpoints",
    create_if_missing=True
)
gto_checkpoint_volume = modal.Volume.from_name(
    "poker-bot-checkpoints",  # Your existing GTO checkpoints
    create_if_missing=True
)

app = modal.App("poker-exploitative-cfr-training")


@app.function(
    image=image,
    volumes={
        "/checkpoints": checkpoint_volume,
        "/gto_checkpoints": gto_checkpoint_volume,
    },
    gpu="T4",  # A10G for faster training
    timeout=36000,  # 10 hours
    memory=16384,  # 16GB RAM
)
def train_exploitative_cfr(
    gto_checkpoint_name: str = "checkpoint_1000.pt",
    num_iterations: int = 1000,
    trajectories_per_iteration: int = 1000,
    batch_size: int = 64,
    learning_rate: float = 5e-5,
    value_learning_rate: float = 5e-5,
    checkpoint_frequency: int = 100,
):
    """Train exploitative Deep CFR agent on Modal.

    This uses the SAME training method as your GTO model,
    but trains to exploit a fixed GTO opponent.

    Args:
        gto_checkpoint_name: Name of GTO checkpoint in Modal volume
        num_iterations: Number of training iterations
        trajectories_per_iteration: Trajectories per iteration
        batch_size: Batch size for network training
        learning_rate: Policy network learning rate
        value_learning_rate: Value network learning rate
        checkpoint_frequency: Save checkpoint every N iterations
    """
    import sys
    sys.path.insert(0, "/root")

    import torch
    import logging
    from tqdm import tqdm

    from poker_game.game import PokerGame
    from poker_game.state_encoder import StateEncoder
    from training.exploitative_trainer import ExploitativeTrainer

    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
    )
    logger = logging.getLogger(__name__)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    print("=" * 80)
    print("EXPLOITATIVE DEEP CFR TRAINING (Best Response)")
    print("=" * 80)
    print(f"GTO Checkpoint: {gto_checkpoint_name}")
    print(f"Iterations: {num_iterations}")
    print(f"Trajectories/Iteration: {trajectories_per_iteration}")
    print(f"Device: {device}")
    print("=" * 80)

    # Initialize game and encoder
    game = PokerGame(small_blind=50, big_blind=100, is_limit=False)
    state_encoder = StateEncoder()

    # Initialize exploitative trainer
    gto_checkpoint_path = f"/gto_checkpoints/{gto_checkpoint_name}"
    trainer = ExploitativeTrainer(
        game=game,
        state_encoder=state_encoder,
        gto_checkpoint_path=gto_checkpoint_path,
        learning_rate=learning_rate,
        value_learning_rate=value_learning_rate,
        device=device,
    )

    logger.info(f"Initialized exploitative trainer vs GTO: {gto_checkpoint_path}")

    # Training loop
    best_avg_payoff = float('-inf')
    metrics_history = []

    for iteration in range(num_iterations):
        logger.info(f"\n{'='*80}")
        logger.info(f"Iteration {iteration + 1}/{num_iterations}")
        logger.info(f"{'='*80}")

        # Train one iteration
        metrics = trainer.train_iteration(
            num_trajectories=trajectories_per_iteration,
            batch_size=batch_size,
        )

        # Log metrics
        logger.info(f"Avg Payoff: {metrics['avg_payoff']:+.2f} chips")
        logger.info(f"Win Rate: {metrics['win_rate']:.2%}")
        logger.info(f"Trajectories: {metrics['num_trajectories']}")
        logger.info(f"Value Buffer: {metrics['value_buffer_size']}")
        logger.info(f"Policy Buffer: {metrics['policy_buffer_size']}")
        logger.info(f"Value Loss: {metrics['value_loss']:.6f}")
        logger.info(f"Policy Loss: {metrics['policy_loss']:.6f}")

        metrics_history.append({
            'iteration': iteration + 1,
            **metrics,
        })

        # Update best payoff tracking
        if metrics['avg_payoff'] > best_avg_payoff:
            best_avg_payoff = metrics['avg_payoff']
            logger.info(f"  ‚≠ê New best avg payoff: {best_avg_payoff:+.2f}")

        # Save checkpoint
        if (iteration + 1) % checkpoint_frequency == 0:
            checkpoint_path = f"/checkpoints/exploitative_cfr_iter_{iteration+1:04d}.pt"
            logger.info(f"Saving checkpoint to {checkpoint_path}")

            trainer.save_checkpoint(
                checkpoint_path,
                iteration=iteration + 1,
                metrics=metrics,
            )

            checkpoint_volume.commit()

            # Save metrics to JSON for plotting
            import json
            metrics_path = f"/checkpoints/metrics_iter_{iteration+1:04d}.json"
            with open(metrics_path, 'w') as f:
                json.dump(metrics_history, f, indent=2)
            checkpoint_volume.commit()
            logger.info(f"  üíæ Saved metrics history to {metrics_path}")

    # Save final checkpoint
    final_path = "/checkpoints/exploitative_cfr_final.pt"
    logger.info(f"\nSaving final checkpoint to {final_path}")
    trainer.save_checkpoint(
        final_path,
        iteration=num_iterations,
        metrics=metrics_history[-1] if metrics_history else {},
    )

    # Always save final metrics (even if no checkpoint was saved during training)
    import json
    final_metrics_path = f"/checkpoints/metrics_final.json"
    with open(final_metrics_path, 'w') as f:
        json.dump(metrics_history, f, indent=2)
    logger.info(f"Saved final metrics to {final_metrics_path}")

    checkpoint_volume.commit()

    # Calculate summary statistics
    all_payoffs = [m['avg_payoff'] for m in metrics_history]
    all_win_rates = [m['win_rate'] for m in metrics_history]
    overall_avg_payoff = sum(all_payoffs) / len(all_payoffs)
    overall_avg_winrate = sum(all_win_rates) / len(all_win_rates)

    # Calculate last N average (smoothed)
    last_n = min(10, len(metrics_history))
    recent_avg_payoff = sum(all_payoffs[-last_n:]) / last_n
    recent_avg_winrate = sum(all_win_rates[-last_n:]) / last_n

    # Print summary
    print("\n" + "="*80)
    print("TRAINING COMPLETE!")
    print("="*80)
    print(f"Best single iteration payoff: {best_avg_payoff:+.2f} chips")
    print(f"Overall average payoff: {overall_avg_payoff:+.2f} chips")
    print(f"Recent average payoff (last {last_n}): {recent_avg_payoff:+.2f} chips")
    print(f"Overall win rate: {overall_avg_winrate:.2%}")
    print(f"Recent win rate (last {last_n}): {recent_avg_winrate:.2%}")

    print("\n" + "="*80)
    print("TRAINING SUMMARY (Last 10 Iterations)")
    print("="*80)
    for metrics in metrics_history[-10:]:
        print(f"Iter {metrics['iteration']:4d}: "
              f"Payoff={metrics['avg_payoff']:+7.2f}, "
              f"WinRate={metrics['win_rate']:6.2%}")

    return {
        "status": "completed",
        "num_iterations": num_iterations,
        "best_single_payoff": best_avg_payoff,
        "overall_avg_payoff": overall_avg_payoff,
        "recent_avg_payoff": recent_avg_payoff,
        "overall_win_rate": overall_avg_winrate,
        "recent_win_rate": recent_avg_winrate,
        "final_win_rate": metrics_history[-1]['win_rate'],
        "metrics_history": metrics_history,
    }


@app.local_entrypoint()
def main(
    gto_checkpoint: str = "checkpoint_1000.pt",
    num_iterations: int = 1000,
    trajectories_per_iteration: int = 1000,
):
    """Local entrypoint to launch exploitative CFR training on Modal."""
    print("\n" + "="*80)
    print("LAUNCHING EXPLOITATIVE DEEP CFR TRAINING ON MODAL")
    print("(Same method as GTO, but trains to exploit GTO opponent)")
    print("="*80)
    print(f"\nConfiguration:")
    print(f"  GTO Checkpoint: {gto_checkpoint}")
    print(f"  Iterations: {num_iterations}")
    print(f"  Trajectories/Iteration: {trajectories_per_iteration}")
    print("\nThis will train a Deep CFR agent that exploits your GTO model")
    print("using the SAME training algorithm (no LLMs, no prompts).")
    print("\nStarting training...")

    result = train_exploitative_cfr.remote(
        gto_checkpoint_name=gto_checkpoint,
        num_iterations=num_iterations,
        trajectories_per_iteration=trajectories_per_iteration,
    )

    print("\n" + "="*80)
    print("TRAINING RESULT")
    print("="*80)
    print(f"Status: {result['status']}")
    print(f"Best Single Iteration: {result['best_single_payoff']:+.2f} chips")
    print(f"Overall Average: {result['overall_avg_payoff']:+.2f} chips")
    print(f"Recent Average: {result['recent_avg_payoff']:+.2f} chips")
    print(f"Overall Win Rate: {result['overall_win_rate']:.2%}")
    print(f"Recent Win Rate: {result['recent_win_rate']:.2%}")
    print(f"\nCheckpoints saved to Modal volume: poker-exploitative-cfr-checkpoints")
    print("\nTo download your trained exploitative model:")
    print("  modal volume get poker-exploitative-cfr-checkpoints \\")
    print("    exploitative_cfr_final.pt \\")
    print("    ./exploitative_cfr_final.pt")
