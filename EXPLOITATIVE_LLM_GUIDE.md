# Exploitative LLM Poker Bot Training Guide

This guide explains how to train an **exploitative LLM poker bot** that can compete against and exploit GTO (Game Theory Optimal) players.

## Overview

The exploitative LLM model is designed to:

1. **Play against your trained Deep CFR GTO agent** as the opponent
2. **Learn unconventional strategies** that exploit predictable GTO play
3. **Use creative tactics** like:
   - Heavy bluffing in unexpected spots
   - Unconventional bet sizing (overbets, tiny bets)
   - Aggressive play to apply pressure
   - Exploiting fold equity

## Architecture

### Components

1. **GTOAgent** (`llm_poker/gto_opponent.py`)
   - Loads your Deep CFR checkpoint
   - Uses average strategy (Nash equilibrium)
   - Acts as the opponent during training

2. **ExploitativePokerEnv** (`llm_poker/exploitative_env.py`)
   - Modified environment with exploitative reward shaping
   - Rewards unconventional plays that exploit GTO patterns
   - Tracks metrics for analysis

3. **Training Script** (`scripts/train_exploitative_llm.py`)
   - Tinker-based RL training loop
   - Configurable reward bonuses
   - Supports checkpointing and resumption

4. **Modal Deployment** (`modal_deploy/train_exploitative_llm.py`)
   - Cloud-based distributed training
   - GPU support for faster training
   - Automatic checkpoint management

### Reward System

The exploitative environment uses a **modified reward system**:

| Reward Type | Default Value | Description |
|-------------|---------------|-------------|
| **Base Payoff** | ¬±1.0 (normalized) | Chips won/lost (normalized to [-1, 1]) |
| **Bluff Bonus** | +0.5 per aggressive action | Winning with weak hands through aggression |
| **Aggression Bonus** | +0.1 per bet/raise | Encourages betting and raising |
| **Fold Equity Bonus** | +0.3 when opponent folds | Making GTO opponent fold |
| **Exploitative Sizing Bonus** | +0.2 per unconventional bet | Overbets (>120% pot), tiny bets (<20% pot) |
| **Showdown Bonus** | +0.2 for showdown wins | Winning at showdown |
| **Illegal Action Penalty** | -0.5 | Invalid/malformed actions |

**Total Reward Formula:**
```
Total Reward = Normalized Payoff + Shaping Rewards + Bonus Rewards
```

This encourages the model to:
- Bluff more aggressively
- Use creative bet sizing
- Apply pressure to exploit GTO's predictable patterns
- Win through fold equity, not just showdown value

---

## Setup Instructions

### Prerequisites

1. **Trained Deep CFR GTO Checkpoint**
   - You need a trained GTO model to use as the opponent
   - Location: `checkpoints/checkpoint_XXXX.pt`
   - This should contain `strategy_memory` and network weights

2. **Tinker API Access**
   - Get your Tinker API key from [tinker.anthropic.com](https://tinker.anthropic.com)
   - Set environment variable: `export TINKER_API_KEY="your-api-key"`

3. **Python Environment**
   ```bash
   # Create conda environment
   conda create -n poker_bot_tinker python=3.10
   conda activate poker_bot_tinker

   # Install dependencies
   pip install torch numpy tqdm
   pip install tinker-client tinker-cookbook
   ```

4. **Modal Setup** (for cloud training)
   ```bash
   pip install modal
   modal setup

   # Create Modal secret for Tinker API key
   modal secret create tinker-api-key TINKER_API_KEY="your-api-key"
   ```

---

## Training Methods

### Method 1: Local Training (Quick Testing)

**Use this for:** Small experiments, testing reward parameters, debugging

```bash
# Activate environment
conda activate poker_bot_tinker
export TINKER_API_KEY="your-api-key"

# Run training
python scripts/train_exploitative_llm.py \
  --gto-checkpoint checkpoints/checkpoint_1000.pt \
  --num-batches 10 \
  --episodes-per-batch 8 \
  --learning-rate 4e-5 \
  --bluff-bonus 0.5 \
  --aggression-bonus 0.1 \
  --fold-equity-bonus 0.3 \
  --exploitative-sizing-bonus 0.2
```

**Parameters:**
- `--gto-checkpoint`: Path to your trained GTO checkpoint (REQUIRED)
- `--num-batches`: Number of training iterations (default: 50)
- `--episodes-per-batch`: Episodes per batch (default: 16)
- `--learning-rate`: LoRA learning rate (default: 4e-5)
- `--bluff-bonus`: Bonus for bluffing (default: 0.5)
- `--aggression-bonus`: Bonus for aggression (default: 0.1)
- `--fold-equity-bonus`: Bonus for fold equity (default: 0.3)
- `--exploitative-sizing-bonus`: Bonus for unconventional sizing (default: 0.2)

**Output:**
- Checkpoints: `/tmp/tinker-examples/rl_poker_exploitative/`
- Logs: Console output + ML logs in checkpoint directory

---

### Method 2: Modal Cloud Training (Recommended)

**Use this for:** Production training runs, longer training, parallel experiments

#### Step 1: Upload GTO Checkpoint to Modal

First, upload your GTO checkpoint to the Modal volume:

```bash
# Create a script to upload checkpoint
modal volume put poker-bot-checkpoints checkpoints/checkpoint_1000.pt checkpoint_1000.pt
```

#### Step 2: Launch Training

```bash
# Navigate to modal_deploy directory
cd modal_deploy

# Launch training on Modal
modal run train_exploitative_llm.py::main \
  --gto-checkpoint checkpoint_1000.pt \
  --num-batches 100 \
  --episodes-per-batch 16
```

**What happens:**
1. Modal provisions a GPU instance (T4)
2. Loads your code and dependencies
3. Downloads GTO checkpoint from Modal volume
4. Runs training for specified batches
5. Saves checkpoints to Modal volume: `poker-exploitative-llm-checkpoints`
6. Automatically shuts down when complete

#### Step 3: Download Trained Model

```bash
# List checkpoints
modal volume ls poker-exploitative-llm-checkpoints

# Download specific checkpoint
modal volume get poker-exploitative-llm-checkpoints \
  exploitative_llm_logs/checkpoints/exploitative_000099_state.pkl \
  ./trained_exploitative_model.pkl
```

---

## Monitoring Training

### Key Metrics to Watch

1. **Mean Reward** - Should increase over time
   - Target: > 0.2 (indicates exploiting GTO)
   - If negative, agent is being exploited

2. **Win Rate** - Percentage of hands won
   - Target: > 55% (exploiting GTO)
   - GTO baseline: ~50% (break-even)

3. **Mean Payoff** - Raw chips won per hand
   - Target: > +100 chips/hand
   - Indicates consistent exploitation

4. **Bluff Bonus** - Average bluff rewards
   - Higher = more successful bluffs
   - Indicates unconventional play

### Tensorboard Monitoring

```bash
# If using Tinker cookbook's ML logging
tensorboard --logdir /tmp/tinker-examples/rl_poker_exploitative/
```

---

## Hyperparameter Tuning

### Reward Bonuses

Experiment with these to encourage different playstyles:

**Aggressive Exploiter:**
```bash
--bluff-bonus 0.8 \
--aggression-bonus 0.2 \
--fold-equity-bonus 0.5 \
--exploitative-sizing-bonus 0.3
```

**Balanced Exploiter:**
```bash
--bluff-bonus 0.5 \
--aggression-bonus 0.1 \
--fold-equity-bonus 0.3 \
--exploitative-sizing-bonus 0.2
```

**Subtle Exploiter:**
```bash
--bluff-bonus 0.3 \
--aggression-bonus 0.05 \
--fold-equity-bonus 0.2 \
--exploitative-sizing-bonus 0.4
```

### Learning Rate

- **Default:** 4e-5 (stable, slower learning)
- **Fast learning:** 8e-5 (less stable, faster adaptation)
- **Very stable:** 2e-5 (very stable, slow learning)

### Training Duration

| Goal | Batches | Episodes/Batch | Total Episodes | Est. Time (Modal) |
|------|---------|----------------|----------------|-------------------|
| Quick test | 10 | 8 | 80 | ~30 min |
| Initial training | 50 | 16 | 800 | ~2-3 hours |
| Full training | 200 | 16 | 3,200 | ~8-12 hours |
| Production | 500 | 32 | 16,000 | ~24-48 hours |

---

## Evaluation

### Test Against GTO

Create an evaluation script to test your exploitative model:

```python
# scripts/eval_exploitative_vs_gto.py
from llm_poker.exploitative_env import ExploitativePokerEnv
# ... load your trained LLM and run episodes
```

### Expected Performance

After training on **200+ batches**:

| Metric | Expected Value | Interpretation |
|--------|----------------|----------------|
| Win Rate vs GTO | 55-65% | Successfully exploiting |
| Mean Payoff | +200 to +500 chips/hand | Significant edge |
| Bluff Success Rate | 40-60% | Effective bluffing |
| Aggression Frequency | 30-50% | High aggression |

### Red Flags

‚ö†Ô∏è **Win rate < 45%** - Agent is being counter-exploited
‚ö†Ô∏è **Mean payoff < -100** - Agent is losing significantly
‚ö†Ô∏è **Bluff bonus = 0** - No unconventional play learned

---

## Architecture Details

### How It Works

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              EXPLOITATIVE LLM TRAINING LOOP             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   1. Reset Environment            ‚îÇ
        ‚îÇ   - New hand dealt                ‚îÇ
        ‚îÇ   - Hero = Player 0 (LLM)         ‚îÇ
        ‚îÇ   - Opponent = Player 1 (GTO)     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   2. LLM Makes Decision           ‚îÇ
        ‚îÇ   - Format state as text prompt   ‚îÇ
        ‚îÇ   - Sample action from LLM        ‚îÇ
        ‚îÇ   - Parse action token            ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   3. Apply Action & Shaping       ‚îÇ
        ‚îÇ   - Execute LLM's action          ‚îÇ
        ‚îÇ   - Calculate immediate rewards:  ‚îÇ
        ‚îÇ     * Aggression bonus            ‚îÇ
        ‚îÇ     * Sizing bonus                ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   4. GTO Opponent Responds        ‚îÇ
        ‚îÇ   - Load GTO checkpoint           ‚îÇ
        ‚îÇ   - Sample from average strategy  ‚îÇ
        ‚îÇ   - Apply GTO action              ‚îÇ
        ‚îÇ   - Track fold equity bonus       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   5. Terminal Reward Calculation  ‚îÇ
        ‚îÇ   - Base: Normalized payoff       ‚îÇ
        ‚îÇ   - Bluff bonus (weak hand wins)  ‚îÇ
        ‚îÇ   - Showdown bonus                ‚îÇ
        ‚îÇ   - Total = Base + Shaping        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   6. RL Update (Tinker)           ‚îÇ
        ‚îÇ   - Create Datum with advantages  ‚îÇ
        ‚îÇ   - Importance sampling loss      ‚îÇ
        ‚îÇ   - LoRA parameter update         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### GTO Opponent

The GTO opponent:
1. Loads your trained Deep CFR checkpoint
2. Uses **average strategy** (strategy_memory) which represents the Nash equilibrium
3. Samples actions according to this equilibrium strategy
4. Provides a consistent, near-optimal baseline to exploit

### Exploitative Strategies Learned

The LLM learns to:

1. **Bluff in unconventional spots**
   - GTO rarely bluffs on certain board textures
   - LLM exploits by over-bluffing these spots

2. **Use creative bet sizing**
   - Overbets (>100% pot) to apply maximum pressure
   - Tiny bets (<25% pot) to induce calls with weak hands

3. **Apply aggression**
   - GTO folds at equilibrium frequencies
   - LLM over-bets to exploit these folds

4. **Exploit predictability**
   - GTO is balanced but predictable
   - LLM finds patterns in GTO's strategy

---

## Troubleshooting

### Issue: "No module named 'tinker'"

**Solution:**
```bash
pip install tinker-client tinker-cookbook
```

### Issue: "GTO checkpoint not found"

**Solution:**
1. Check checkpoint path exists: `ls checkpoints/`
2. Train GTO model first: `modal run modal_deploy/train.py`
3. Use correct checkpoint name in `--gto-checkpoint`

### Issue: "TINKER_API_KEY not set"

**Solution:**
```bash
export TINKER_API_KEY="your-api-key-here"
```

For Modal:
```bash
modal secret create tinker-api-key TINKER_API_KEY="your-api-key"
```

### Issue: Training loss not decreasing

**Possible causes:**
1. Learning rate too low - Try `--learning-rate 8e-5`
2. Not enough episodes - Increase `--episodes-per-batch`
3. Reward bonuses too small - Increase bonus values

### Issue: Win rate < 45% (being exploited)

**Possible causes:**
1. GTO checkpoint is too weak - Train GTO for more iterations
2. Reward bonuses encouraging bad play - Reduce bonus values
3. Not enough training - Increase `--num-batches`

---

## Advanced Topics

### Custom Reward Functions

Edit `llm_poker/exploitative_env.py` to add custom rewards:

```python
# Example: Reward for check-raising (trappy play)
if previous_action == Action.CHECK and action == Action.RAISE:
    immediate_reward += 0.3  # Check-raise bonus
```

### Multi-Opponent Training

Train against multiple GTO checkpoints for robustness:

```python
# Modify exploitative_env.py
checkpoints = [
    "/path/to/checkpoint_500.pt",
    "/path/to/checkpoint_1000.pt",
    "/path/to/checkpoint_2000.pt",
]
# Randomly select checkpoint per episode
```

### Curriculum Learning

Start with weaker GTO opponents, gradually increase difficulty:

```python
# Batches 0-50: checkpoint_100.pt (weak GTO)
# Batches 50-100: checkpoint_500.pt (medium GTO)
# Batches 100+: checkpoint_1000.pt (strong GTO)
```

---

## Next Steps

1. **Train initial GTO model**
   ```bash
   modal run modal_deploy/train.py
   ```

2. **Run quick exploitative test**
   ```bash
   python scripts/train_exploitative_llm.py \
     --gto-checkpoint checkpoints/checkpoint_1000.pt \
     --num-batches 10
   ```

3. **Launch full Modal training**
   ```bash
   modal run modal_deploy/train_exploitative_llm.py::main \
     --gto-checkpoint checkpoint_1000.pt \
     --num-batches 200
   ```

4. **Evaluate and iterate**
   - Monitor win rate and payoff
   - Adjust reward bonuses
   - Re-train with better parameters

---

## Cost Estimates (Modal)

| Training Duration | GPU Type | Est. Cost |
|-------------------|----------|-----------|
| 10 batches (test) | T4 | ~$0.50 |
| 100 batches | T4 | ~$3-5 |
| 500 batches | T4 | ~$15-20 |

**Tinker API costs** are separate and depend on usage.

---

## Questions?

For issues or questions:
1. Check existing GTO checkpoints in `checkpoints/`
2. Review logs in `/tmp/tinker-examples/rl_poker_exploitative/`
3. Test with small `--num-batches` first
4. Verify Tinker API key is set correctly

Happy exploiting! üÉèüé∞
