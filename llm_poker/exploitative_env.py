"""Exploitative poker environment with modified reward system.

This environment is designed to train an LLM agent to exploit GTO players
by rewarding unconventional plays like:
- Bluffing in unexpected spots
- Over-betting and under-betting
- Aggressive play
- Winning through fold equity
"""

from dataclasses import dataclass
from typing import Optional, Tuple, List
import random

from poker_game.game import PokerGame, GameState, Action
from llm_poker.text_interface import (
    ActionOption,
    build_action_options,
    format_state_prompt,
    parse_action_token,
)
from llm_poker.gto_opponent import GTOAgent


@dataclass
class StepResult:
    """Result of a single environment step."""
    prompt: str
    reward: float
    done: bool
    info: dict


class ExploitativePokerEnv:
    """Poker environment optimized for training exploitative play vs GTO.

    Key differences from standard PokerTextEnv:
    1. Opponent is a GTO agent (Deep CFR)
    2. Modified reward system that encourages:
       - Bluffing (winning with weak hands)
       - Aggressive play (betting/raising)
       - Fold equity (making opponents fold)
       - Unconventional bet sizing
    3. Additional shaping rewards during play
    """

    def __init__(
        self,
        small_blind: int = 50,
        big_blind: int = 100,
        starting_stack: int = 20000,
        gto_checkpoint_path: str = None,

        # Reward parameters
        bluff_bonus: float = 0.5,  # Bonus for winning with weak hand via bluff
        aggression_bonus: float = 0.1,  # Bonus for betting/raising
        fold_equity_bonus: float = 0.3,  # Bonus for making opponent fold
        showdown_bonus: float = 0.2,  # Bonus for winning at showdown
        exploitative_sizing_bonus: float = 0.2,  # Bonus for unusual bet sizes

        # Standard penalties
        illegal_penalty: float = -0.5,
        payoff_scale: float = 20000.0,
    ):
        self.game = PokerGame(
            small_blind=small_blind,
            big_blind=big_blind,
            is_limit=False,
            starting_stack=starting_stack,
        )

        # Reward parameters
        self.bluff_bonus = bluff_bonus
        self.aggression_bonus = aggression_bonus
        self.fold_equity_bonus = fold_equity_bonus
        self.showdown_bonus = showdown_bonus
        self.exploitative_sizing_bonus = exploitative_sizing_bonus
        self.illegal_penalty = illegal_penalty
        self.payoff_scale = payoff_scale

        # Initialize GTO opponent
        self.gto_opponent = GTOAgent(
            game=self.game,
            checkpoint_path=gto_checkpoint_path,
            device='cpu'
        )

        # Internal state
        self.state: Optional[GameState] = None
        self.last_action_options: List[ActionOption] = []

        # Track actions for reward shaping
        self.hero_actions: List[Tuple[Action, int]] = []
        self.initial_hand_strength: float = 0.0

    def _evaluate_hand_strength(self, hole_cards, community_cards) -> float:
        """Rough hand strength heuristic in [0, 1]."""
        ranks = [card[0] for card in hole_cards]

        # Pocket pairs
        if ranks[0] == ranks[1]:
            return 0.6 + min(ranks[0], 12) / 12.0 * 0.3

        max_rank = max(ranks)
        if max_rank >= 10:  # Face cards
            return 0.4 + (max_rank - 10) / 2.0 * 0.2

        # Suited
        if hole_cards[0][1] == hole_cards[1][1]:
            return 0.3

        return 0.2

    def _is_aggressive_action(self, action: Action) -> bool:
        """Check if action is aggressive (bet/raise)."""
        return action in (Action.BET, Action.RAISE)

    def _is_unconventional_sizing(
        self,
        action: Action,
        amount: int,
        pot_size: int,
        legal_actions: List[Tuple[Action, int]]
    ) -> bool:
        """Check if bet sizing is unconventional (not standard GTO sizes).

        GTO typically uses 33%, 50%, 75%, 100% pot bets.
        Unconventional: overbet (>100%), tiny bets (<25%), etc.
        """
        if action not in (Action.BET, Action.RAISE):
            return False

        if pot_size == 0:
            return False

        ratio = amount / pot_size

        # Unconventional if:
        # - Overbet (>1.2x pot)
        # - Tiny bet (<0.2x pot)
        # - Unusual sizes (not near standard sizes)
        if ratio > 1.2:  # Overbet
            return True
        if ratio < 0.2:  # Tiny bet
            return True

        # Check if it's not near standard sizes
        standard_sizes = [0.33, 0.5, 0.75, 1.0]
        near_standard = any(abs(ratio - size) < 0.1 for size in standard_sizes)

        return not near_standard

    @property
    def current_player(self) -> int:
        """Return the index of the player whose turn it is."""
        if self.state is None:
            return 0
        return self.state.current_player if self.state.current_player is not None else 0

    def reset(self) -> str:
        """Start a new hand and return the initial prompt for the acting player."""
        self.state = self.game.reset()
        self.last_action_options = build_action_options(self.game, self.state)
        self.hero_actions = []

        # Store initial hand strength for bluff detection
        self.initial_hand_strength = self._evaluate_hand_strength(
            self.state.hole_cards[0],
            []
        )

        prompt = format_state_prompt(self.game, self.state, self.last_action_options)
        return prompt

    def step(self, action_token: str) -> StepResult:
        """Apply the agent's action token and advance the game.

        This includes:
        1. Parse and validate action
        2. Apply action
        3. Let GTO opponent respond
        4. Calculate exploitative rewards
        5. Return result
        """
        if self.state is None:
            raise RuntimeError("Environment must be reset() before step().")

        # Parse action token
        parsed = parse_action_token(action_token, self.last_action_options)
        if parsed is None:
            # Invalid action - immediate penalty
            info = {"error": "invalid_action_token", "token": action_token}
            return StepResult(prompt="", reward=self.illegal_penalty, done=True, info=info)

        action, amount = parsed
        state = self.state

        # Track hero's action
        self.hero_actions.append((action, amount))

        # Calculate immediate shaping rewards
        immediate_reward = 0.0

        # Aggression bonus
        if self._is_aggressive_action(action):
            immediate_reward += self.aggression_bonus

        # Unconventional sizing bonus
        legal_actions = self.game.get_legal_actions(state)
        if self._is_unconventional_sizing(action, amount, state.pot, legal_actions):
            immediate_reward += self.exploitative_sizing_bonus

        # Apply hero's action
        state = self.game.apply_action(state, action, amount)

        # Check if hand ended
        if state.is_terminal:
            return self._handle_terminal(state, immediate_reward)

        # Let GTO opponent act
        while (not state.is_terminal) and state.current_player == 1:
            legal_actions = self.game.get_legal_actions(state)
            if not legal_actions:
                break

            opp_action, opp_amount = self.gto_opponent.get_action(state, legal_actions)

            # Check if opponent folded (fold equity bonus!)
            if opp_action == Action.FOLD:
                immediate_reward += self.fold_equity_bonus

            state = self.game.apply_action(state, opp_action, opp_amount)

        # Check if hand ended after opponent action
        if state.is_terminal:
            return self._handle_terminal(state, immediate_reward)

        # Hand continues - prepare next prompt
        self.state = state
        self.last_action_options = build_action_options(self.game, state)
        prompt = format_state_prompt(self.game, state, self.last_action_options)

        return StepResult(prompt=prompt, reward=immediate_reward, done=False, info={})

    def _handle_terminal(self, state: GameState, accumulated_reward: float) -> StepResult:
        """Handle terminal state and calculate final rewards."""
        self.state = state
        payoffs = self.game.get_payoff(state)

        # Normalize payoff
        raw_payoff = payoffs[0]
        normalized_payoff = max(-self.payoff_scale, min(self.payoff_scale, raw_payoff)) / self.payoff_scale

        # Bonus rewards for exploitative play
        bonus_reward = 0.0

        # Check if we won
        if raw_payoff > 0:
            # Showdown bonus
            if state.show_down:
                bonus_reward += self.showdown_bonus

            # Bluff bonus - won with a weak hand
            current_hand_strength = self._evaluate_hand_strength(
                state.hole_cards[0],
                state.community_cards
            )

            # If we won with a weak hand through aggression (bluffing), big bonus!
            if current_hand_strength < 0.4:
                # Check if we were aggressive
                aggressive_actions = sum(
                    1 for a, _ in self.hero_actions
                    if self._is_aggressive_action(a)
                )
                if aggressive_actions > 0:
                    bonus_reward += self.bluff_bonus * aggressive_actions

        # Total reward = normalized payoff + accumulated shaping + bonuses
        total_reward = normalized_payoff + accumulated_reward + bonus_reward

        info = {
            "payoffs": payoffs,
            "normalized_payoff": normalized_payoff,
            "shaping_reward": accumulated_reward,
            "bonus_reward": bonus_reward,
            "total_reward": total_reward,
            "hero_actions": self.hero_actions,
        }

        return StepResult(prompt="", reward=total_reward, done=True, info=info)
