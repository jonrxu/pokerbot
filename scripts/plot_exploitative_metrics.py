#!/usr/bin/env python3
"""Plot exploitative CFR training metrics.

This script downloads and visualizes training metrics from Modal,
showing how well the exploitative agent is learning to beat the GTO opponent.
"""

import json
import matplotlib.pyplot as plt
import numpy as np
import argparse
from pathlib import Path


def plot_exploitative_metrics(
    metrics_file: str = None,
    output_file: str = 'exploitative_training.png',
    show_plot: bool = False
):
    """Plot exploitative training metrics.

    Args:
        metrics_file: Path to metrics JSON file (if None, downloads from Modal)
        output_file: Output image path
        show_plot: Whether to display plot
    """
    # Load metrics
    if metrics_file is None:
        # Download from Modal
        print("Downloading metrics from Modal...")
        import subprocess
        result = subprocess.run(
            [
                'modal', 'volume', 'ls',
                'poker-exploitative-cfr-checkpoints'
            ],
            capture_output=True,
            text=True
        )

        # Find metrics files (look for both _final and _iter_ files)
        all_files = result.stdout.split('\n')
        metrics_files = [
            line.split()[0] for line in all_files
            if 'metrics_' in line and '.json' in line
        ]

        if not metrics_files:
            print("No metrics files found in Modal volume!")
            print("Run training first, or wait for checkpoints to be saved.")
            return

        # Prefer _final.json if it exists, otherwise use latest _iter_ file
        final_metrics = [f for f in metrics_files if 'metrics_final.json' in f]
        if final_metrics:
            latest = final_metrics[0]
            print(f"Found final metrics: {latest}")
        else:
            iter_metrics = [f for f in metrics_files if 'metrics_iter_' in f]
            if iter_metrics:
                latest = sorted(iter_metrics)[-1]
                print(f"Found latest iteration metrics: {latest}")
            else:
                print("No metrics files found!")
                return

        # Download it (with --force to overwrite existing file)
        subprocess.run([
            'modal', 'volume', 'get',
            '--force',
            'poker-exploitative-cfr-checkpoints',
            latest,
            './temp_metrics.json',
            '--force'
        ])

        metrics_file = './temp_metrics.json'

    # Load metrics
    with open(metrics_file, 'r') as f:
        metrics = json.load(f)

    if not metrics:
        print("No metrics found!")
        return

    # Extract data
    iterations = [m['iteration'] for m in metrics]
    avg_payoffs = [m['avg_payoff'] for m in metrics]
    win_rates = [m['win_rate'] * 100 for m in metrics]  # Convert to percentage

    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle(
        'Exploitative CFR Training: Learning to Beat GTO',
        fontsize=16,
        fontweight='bold'
    )

    # Plot 1: Average Payoff (Primary metric)
    ax1.plot(iterations, avg_payoffs, 'b-o', linewidth=2.5, markersize=5,
            label='Avg Payoff', alpha=0.8)
    ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Break Even')
    ax1.set_xlabel('Iteration', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Avg Payoff (chips/hand)', fontsize=12, color='b', fontweight='bold')
    ax1.set_title('Average Payoff vs GTO Opponent', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3, linestyle='--')
    ax1.tick_params(axis='y', labelcolor='b')
    ax1.legend(loc='lower right', fontsize=10)

    # Add trend line
    if len(iterations) > 1:
        z = np.polyfit(iterations, avg_payoffs, 1)
        p = np.poly1d(z)
        ax1.plot(iterations, p(iterations), 'g--', alpha=0.6, linewidth=2,
                label=f'Trend (slope: {z[0]:.2f} chips/iter)')
        ax1.legend(loc='lower right', fontsize=10)

    # Add stats box
    if avg_payoffs:
        current = avg_payoffs[-1]
        best = max(avg_payoffs)
        improvement = current - avg_payoffs[0] if avg_payoffs[0] != 0 else 0

        stats_text = (
            f'Current: {current:+.1f} chips/hand\n'
            f'Best: {best:+.1f} chips/hand\n'
            f'Improvement: {improvement:+.1f} chips'
        )

        # Color based on performance
        if current > 100:
            box_color = 'lightgreen'
            performance = 'ðŸ”¥ Excellent!'
        elif current > 50:
            box_color = 'lightblue'
            performance = 'âœ“ Good'
        elif current > 0:
            box_color = 'lightyellow'
            performance = 'âš  Marginal'
        else:
            box_color = 'lightcoral'
            performance = 'âœ— Losing'

        stats_text = f'{performance}\n\n{stats_text}'

        ax1.text(0.02, 0.98, stats_text,
                transform=ax1.transAxes, fontsize=10,
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor=box_color, alpha=0.8))

    # Plot 2: Win Rate
    ax2.plot(iterations, win_rates, 'g-o', linewidth=2.5, markersize=5,
            label='Win Rate', alpha=0.8)
    ax2.axhline(y=50, color='r', linestyle='--', alpha=0.5, label='50% (Break Even)')
    ax2.set_xlabel('Iteration', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Win Rate (%)', fontsize=12, color='g', fontweight='bold')
    ax2.set_title('Win Rate vs GTO Opponent', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3, linestyle='--')
    ax2.tick_params(axis='y', labelcolor='g')
    ax2.set_ylim([min(win_rates) - 5, max(win_rates) + 5])
    ax2.legend(loc='lower right', fontsize=10)

    # Add win rate info
    if win_rates:
        current_wr = win_rates[-1]
        best_wr = max(win_rates)

        wr_text = (
            f'Current: {current_wr:.1f}%\n'
            f'Best: {best_wr:.1f}%\n'
        )

        # Interpretation
        if current_wr > 60:
            interpretation = 'ðŸ”¥ Crushing it!'
        elif current_wr > 55:
            interpretation = 'âœ“ Strong edge'
        elif current_wr > 52:
            interpretation = 'âš  Small edge'
        elif current_wr > 50:
            interpretation = 'âš  Marginal'
        else:
            interpretation = 'âœ— Needs work'

        wr_text = f'{interpretation}\n\n{wr_text}'

        ax2.text(0.02, 0.98, wr_text,
                transform=ax2.transAxes, fontsize=10,
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))

    # Add overall stats at bottom
    stats_text = f'Total Iterations: {len(iterations)} | Final Payoff: {avg_payoffs[-1]:+.1f} chips | Final Win Rate: {win_rates[-1]:.1f}%'
    fig.text(0.5, 0.01, stats_text, ha='center', fontsize=10, style='italic')

    plt.tight_layout()
    plt.subplots_adjust(bottom=0.05)

    # Calculate overall statistics
    overall_avg_payoff = sum(avg_payoffs) / len(avg_payoffs) if avg_payoffs else 0
    overall_avg_winrate = sum(win_rates) / len(win_rates) if win_rates else 0

    # Save figure
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f'âœ“ Graph saved to: {output_file}')
    print(f'  Iterations: {len(iterations)}')
    print(f'  Overall Avg Payoff: {overall_avg_payoff:+.1f} chips/hand')
    print(f'  Final Iteration Payoff: {avg_payoffs[-1]:+.1f} chips/hand')
    print(f'  Overall Win Rate: {overall_avg_winrate:.1f}%')
    print(f'  Final Win Rate: {win_rates[-1]:.1f}%')

    # Provide interpretation
    print()
    print("=" * 80)
    print("INTERPRETATION")
    print("=" * 80)

    # Use overall average for interpretation, not just final iteration
    if len(iterations) < 50:
        print(f"âš ï¸ SMALL SAMPLE: Only {len(iterations)} iterations")
        print(f"   Results are very noisy. Need 100+ iterations for reliable signals.")
        print()
        if overall_avg_payoff > 0:
            print(f"âœ“ SLIGHTLY POSITIVE: {overall_avg_payoff:+.1f} chips/hand overall")
            print(f"  This is promising but not statistically significant yet.")
        else:
            print(f"âš  SLIGHTLY NEGATIVE: {overall_avg_payoff:+.1f} chips/hand overall")
            print(f"  Too early to tell if this is real or just variance.")
    elif overall_avg_payoff > 100:
        print("ðŸ”¥ EXCELLENT! Your model is crushing the GTO opponent!")
        print("   This is a very strong exploitative strategy.")
    elif overall_avg_payoff > 50:
        print("âœ“ GOOD! Your model has found a solid edge against GTO.")
        print("  This is a profitable exploitative strategy.")
    elif overall_avg_payoff > 0:
        print("âš  MARGINAL: Your model is slightly beating GTO.")
        print("  Consider training longer or tuning hyperparameters.")
    else:
        print("âœ— PROBLEM: Your model is losing to GTO on average!")
        print("  Check your training setup or train longer.")

    print()
    print("Avg Payoff Guide:")
    print("  200+ chips/hand: Elite exploitative play")
    print("  100-200: Strong exploitation")
    print("  50-100: Moderate exploitation")
    print("  0-50: Weak exploitation")
    print("  <0: Something is wrong")

    if show_plot:
        plt.show()
    else:
        plt.close()


def main():
    """CLI for plotting exploitative metrics."""
    parser = argparse.ArgumentParser(
        description='Plot exploitative CFR training metrics'
    )
    parser.add_argument(
        '--metrics', type=str, default=None,
        help='Path to metrics JSON file (default: download from Modal)'
    )
    parser.add_argument(
        '--output', type=str, default='exploitative_training.png',
        help='Output image path (default: exploitative_training.png)'
    )
    parser.add_argument(
        '--show', action='store_true',
        help='Display plot after saving'
    )

    args = parser.parse_args()

    plot_exploitative_metrics(
        metrics_file=args.metrics,
        output_file=args.output,
        show_plot=args.show
    )


if __name__ == '__main__':
    main()
