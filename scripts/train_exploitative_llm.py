#!/usr/bin/env python3
"""
Exploitative LLM Poker Bot Training Script

This script trains an LLM agent to exploit GTO players by:
1. Playing against a Deep CFR GTO opponent
2. Receiving rewards for unconventional, exploitative plays
3. Learning to bluff in unexpected spots and use creative bet sizing

Run this with TINKER_API_KEY set in the poker_bot_tinker environment.
"""

import argparse
import logging
import os
from typing import List, Tuple, Dict

import numpy as np
import torch
import tinker
from tinker import types
from tinker.types.tensor_data import TensorData
from tinker_cookbook import checkpoint_utils, model_info, renderers
from tinker_cookbook.tokenizer_utils import get_tokenizer
from tinker_cookbook.utils import ml_log

from llm_poker.exploitative_env import ExploitativePokerEnv, StepResult


logger = logging.getLogger(__name__)


def parse_first_token(text: str) -> str:
    """Extract the first whitespace-delimited token from a model response."""
    text = text.strip()
    if not text:
        return "A0"
    return text.split()[0]


def collect_exploitative_batch_episodes(
    env: ExploitativePokerEnv,
    sampling_client: tinker.SamplingClient,
    renderer,
    batch_size: int,
    max_tokens: int,
) -> Tuple[List[types.Datum], List[float], Dict[str, float]]:
    """Collect a batch of episodes vs GTO opponent and convert to Tinker Datums.

    Returns:
        - List of Datum objects for training
        - List of total episode rewards
        - Dictionary of metrics for logging
    """
    sampling_params = types.SamplingParams(
        max_tokens=max_tokens,
        stop=renderer.get_stop_sequences(),
    )

    all_datums: List[types.Datum] = []
    episode_rewards: List[float] = []

    # Track metrics
    total_payoffs = []
    total_bluff_bonuses = []
    total_aggression_bonuses = []
    total_fold_equity_bonuses = []
    wins = 0
    losses = 0

    for _ in range(batch_size):
        prompt = env.reset()
        done = False
        episode_reward = 0.0

        # Store decisions: (prompt_tokens, sampled_tokens, sampled_logprobs)
        decision_records: List[Tuple[List[int], List[int], List[float]]] = []

        while not done:
            convo = [{"role": "user", "content": prompt}]
            model_input = renderer.build_generation_prompt(convo)
            prompt_tokens = model_input.to_ints()

            # Sample a response
            future = sampling_client.sample(
                prompt=model_input,
                num_samples=1,
                sampling_params=sampling_params,
            )
            sample_result = future.result()
            seq = sample_result.sequences[0]
            sampled_tokens = seq.tokens
            sampled_logprobs = seq.logprobs
            if sampled_logprobs is None:
                sampled_logprobs = [0.0] * len(sampled_tokens)

            # Parse token and step env
            parsed_message, _ = renderer.parse_response(sampled_tokens)
            raw_text = parsed_message["content"]
            action_token = parse_first_token(raw_text)

            step_result: StepResult = env.step(action_token)
            episode_reward += step_result.reward
            done = step_result.done
            prompt = step_result.prompt

            # Record decision
            decision_records.append((prompt_tokens, sampled_tokens, sampled_logprobs))

            # Track metrics at terminal
            if done and "payoffs" in step_result.info:
                payoff = step_result.info["payoffs"][0]
                total_payoffs.append(payoff)

                if payoff > 0:
                    wins += 1
                elif payoff < 0:
                    losses += 1

                # Track bonus rewards
                bonus = step_result.info.get("bonus_reward", 0.0)
                if bonus > 0:
                    total_bluff_bonuses.append(bonus)

        episode_rewards.append(episode_reward)

        # Convert decisions into Datums
        for prompt_tokens, sampled_tokens, sampled_logprobs in decision_records:
            tokens = prompt_tokens + sampled_tokens
            ob_len = len(prompt_tokens) - 1

            input_tokens = tokens[:-1]
            target_tokens = tokens[1:]

            # Align logprobs and advantages
            all_logprobs = [0.0] * ob_len + sampled_logprobs
            all_advantages = [0.0] * ob_len + [episode_reward] * (len(input_tokens) - ob_len)

            assert (
                len(input_tokens)
                == len(target_tokens)
                == len(all_logprobs)
                == len(all_advantages)
            ), "Token/logprob/advantage lengths must match"

            datum = types.Datum(
                model_input=types.ModelInput.from_ints(tokens=input_tokens),
                loss_fn_inputs={
                    "target_tokens": TensorData.from_torch(torch.tensor(target_tokens)),
                    "logprobs": TensorData.from_torch(torch.tensor(all_logprobs)),
                    "advantages": TensorData.from_torch(torch.tensor(all_advantages)),
                },
            )
            all_datums.append(datum)

    # Calculate metrics
    metrics = {
        "mean_reward": float(np.mean(episode_rewards)) if episode_rewards else 0.0,
        "mean_payoff": float(np.mean(total_payoffs)) if total_payoffs else 0.0,
        "win_rate": wins / batch_size if batch_size > 0 else 0.0,
        "loss_rate": losses / batch_size if batch_size > 0 else 0.0,
        "mean_bluff_bonus": float(np.mean(total_bluff_bonuses)) if total_bluff_bonuses else 0.0,
    }

    return all_datums, episode_rewards, metrics


def run_exploitative_rl(
    model_name: str,
    log_path: str,
    gto_checkpoint_path: str,
    num_batches: int,
    episodes_per_batch: int,
    learning_rate: float,
    max_tokens: int,
    lora_rank: int,
    base_url: str | None,

    # Reward shaping parameters
    bluff_bonus: float,
    aggression_bonus: float,
    fold_equity_bonus: float,
    exploitative_sizing_bonus: float,
) -> None:
    """Run the exploitative RL training loop."""
    os.makedirs(log_path, exist_ok=True)

    # Logging
    ml_logger = ml_log.setup_logging(
        log_dir=log_path,
        wandb_project=None,
        wandb_name=None,
        config={
            "model_name": model_name,
            "episodes_per_batch": episodes_per_batch,
            "num_batches": num_batches,
            "type": "exploitative_vs_gto",
            "learning_rate": learning_rate,
            "bluff_bonus": bluff_bonus,
            "aggression_bonus": aggression_bonus,
            "fold_equity_bonus": fold_equity_bonus,
            "exploitative_sizing_bonus": exploitative_sizing_bonus,
        },
        do_configure_logging_module=True,
    )

    # Tokenizer + renderer
    tokenizer = get_tokenizer(model_name)
    renderer_name = model_info.get_recommended_renderer_name(model_name)
    renderer = renderers.get_renderer(renderer_name, tokenizer)
    logger.info(f"Using renderer: {renderer_name}")

    # Tinker clients
    service_client = tinker.ServiceClient(base_url=base_url)

    resume_info = checkpoint_utils.get_last_checkpoint(log_path)
    if resume_info:
        training_client = service_client.create_training_client_from_state(
            resume_info["state_path"]
        )
        start_batch = resume_info["loop_state"].get("batch", 0)
        logger.info(f"Resuming from batch {start_batch}")
    else:
        training_client = service_client.create_lora_training_client(
            base_model=model_name,
            rank=lora_rank,
        )
        start_batch = 0
        logger.info("Starting new LoRA training client for exploitative play")

    # Initial weights for sampling
    sampling_path = (
        training_client.save_weights_for_sampler(name="init")
        .result()
        .path
    )
    sampling_client = service_client.create_sampling_client(model_path=sampling_path)

    adam_params = types.AdamParams(
        learning_rate=learning_rate, beta1=0.9, beta2=0.95, eps=1e-8
    )

    # Exploitative environment vs GTO
    env = ExploitativePokerEnv(
        gto_checkpoint_path=gto_checkpoint_path,
        bluff_bonus=bluff_bonus,
        aggression_bonus=aggression_bonus,
        fold_equity_bonus=fold_equity_bonus,
        exploitative_sizing_bonus=exploitative_sizing_bonus,
    )
    logger.info(f"Initialized exploitative environment vs GTO checkpoint: {gto_checkpoint_path}")

    for batch_idx in range(start_batch, num_batches):
        logger.info(f"Collecting batch {batch_idx+1}/{num_batches} episodes (Exploitative vs GTO)...")

        datums, episode_rewards, batch_metrics = collect_exploitative_batch_episodes(
            env=env,
            sampling_client=sampling_client,
            renderer=renderer,
            batch_size=episodes_per_batch,
            max_tokens=max_tokens,
        )

        # Log metrics
        metrics: dict[str, float] = {
            "progress/batch": batch_idx,
            "progress/done_frac": (batch_idx + 1) / num_batches,
            "optim/lr": learning_rate,
            **batch_metrics,
        }

        logger.info(
            f"Batch {batch_idx}: "
            f"mean_reward={batch_metrics['mean_reward']:.4f}, "
            f"win_rate={batch_metrics['win_rate']:.2%}, "
            f"mean_payoff={batch_metrics['mean_payoff']:.2f}"
        )

        # Training step
        fwd_bwd_future = training_client.forward_backward(
            datums,
            loss_fn="importance_sampling",
        )
        optim_step_future = training_client.optim_step(adam_params)
        _ = fwd_bwd_future.result()
        _ = optim_step_future.result()

        # Save checkpoint and refresh sampling weights
        if (batch_idx + 1) % 2 == 0 or batch_idx == num_batches - 1:
            checkpoint_utils.save_checkpoint(
                training_client=training_client,
                name=f"exploitative_{batch_idx:06d}",
                log_path=log_path,
                kind="state",
                loop_state={"batch": batch_idx + 1},
            )
            sampling_path = (
                training_client.save_weights_for_sampler(
                    name=f"exploitative_{batch_idx:06d}"
                )
                .result()
                .path
            )
            sampling_client = service_client.create_sampling_client(
                model_path=sampling_path
            )

        ml_logger.log_metrics(metrics, step=batch_idx)

    ml_logger.close()
    logger.info("Exploitative poker LLM training completed!")


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Train exploitative LLM poker bot vs GTO via Tinker."
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default="Qwen/Qwen3-4B-Instruct-2507",
        help="Base model to fine-tune via LoRA.",
    )
    parser.add_argument(
        "--log-path",
        type=str,
        default="/tmp/tinker-examples/rl_poker_exploitative",
        help="Directory for Tinker logs and checkpoints.",
    )
    parser.add_argument(
        "--gto-checkpoint",
        type=str,
        required=True,
        help="Path to Deep CFR GTO checkpoint to use as opponent.",
    )
    parser.add_argument(
        "--num-batches",
        type=int,
        default=50,
        help="Number of RL batches (iterations) to run.",
    )
    parser.add_argument(
        "--episodes-per-batch",
        type=int,
        default=16,
        help="Number of episodes per batch.",
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=4e-5,
        help="Learning rate for LoRA optimizer.",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=32,
        help="Max tokens to sample per decision.",
    )
    parser.add_argument(
        "--lora-rank",
        type=int,
        default=16,
        help="LoRA rank for Tinker training client.",
    )
    parser.add_argument(
        "--base-url",
        type=str,
        default=None,
        help="Optional custom Tinker base URL.",
    )

    # Reward shaping parameters
    parser.add_argument(
        "--bluff-bonus",
        type=float,
        default=0.5,
        help="Bonus for winning with weak hands (bluffing).",
    )
    parser.add_argument(
        "--aggression-bonus",
        type=float,
        default=0.1,
        help="Bonus for aggressive actions (bet/raise).",
    )
    parser.add_argument(
        "--fold-equity-bonus",
        type=float,
        default=0.3,
        help="Bonus for making opponent fold.",
    )
    parser.add_argument(
        "--exploitative-sizing-bonus",
        type=float,
        default=0.2,
        help="Bonus for unconventional bet sizing.",
    )

    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    run_exploitative_rl(
        model_name=args.model_name,
        log_path=args.log_path,
        gto_checkpoint_path=args.gto_checkpoint,
        num_batches=args.num_batches,
        episodes_per_batch=args.episodes_per_batch,
        learning_rate=args.learning_rate,
        max_tokens=args.max_tokens,
        lora_rank=args.lora_rank,
        base_url=args.base_url,
        bluff_bonus=args.bluff_bonus,
        aggression_bonus=args.aggression_bonus,
        fold_equity_bonus=args.fold_equity_bonus,
        exploitative_sizing_bonus=args.exploitative_sizing_bonus,
    )


if __name__ == "__main__":
    main()
