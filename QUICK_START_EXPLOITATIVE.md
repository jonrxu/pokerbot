# Quick Start: Exploitative LLM Poker Bot

**Goal:** Train an LLM that exploits GTO players through unconventional strategies like heavy bluffing and creative bet sizing.

---

## ğŸš€ Quick Start (5 Minutes)

### Prerequisites Checklist

- [ ] Trained GTO checkpoint exists in `checkpoints/` directory
- [ ] Tinker API key obtained from [tinker.anthropic.com](https://tinker.anthropic.com)
- [ ] Python 3.10+ environment ready

### Step 1: Setup Environment

```bash
# Create environment
conda create -n poker_bot_tinker python=3.10 -y
conda activate poker_bot_tinker

# Install dependencies
pip install torch numpy tqdm tinker-client tinker-cookbook

# Set API key
export TINKER_API_KEY="your-api-key-here"
```

### Step 2: Quick Local Test

```bash
# Run a quick 10-batch test
python scripts/train_exploitative_llm.py \
  --gto-checkpoint checkpoints/checkpoint_1000.pt \
  --num-batches 10 \
  --episodes-per-batch 8
```

**Expected output:**
- Training starts immediately
- Shows batch progress with win rate and payoff metrics
- Saves checkpoints to `/tmp/tinker-examples/rl_poker_exploitative/`
- Should complete in ~30-60 minutes

---

## â˜ï¸ Modal Cloud Training (Recommended)

### Step 1: Setup Modal

```bash
# Install Modal
pip install modal

# Authenticate
modal setup

# Create Tinker API secret
modal secret create tinker-api-key TINKER_API_KEY="your-api-key"
```

### Step 2: Upload GTO Checkpoint

```bash
# Upload your GTO checkpoint to Modal
modal volume put poker-bot-checkpoints \
  checkpoints/checkpoint_1000.pt \
  checkpoint_1000.pt
```

### Step 3: Launch Training

```bash
# Run full training on Modal
modal run modal_deploy/train_exploitative_llm.py::main \
  --gto-checkpoint checkpoint_1000.pt \
  --num-batches 100
```

### Step 4: Download Results

```bash
# List checkpoints
modal volume ls poker-exploitative-llm-checkpoints

# Download trained model
modal volume get poker-exploitative-llm-checkpoints \
  exploitative_llm_logs/checkpoints/exploitative_000099_state.pkl \
  ./exploitative_model.pkl
```

---

## ğŸ“Š What to Expect

### Training Metrics

| Metric | Initial | After 50 Batches | After 200 Batches |
|--------|---------|------------------|-------------------|
| Win Rate | ~50% | ~52-55% | ~55-65% |
| Mean Payoff | ~0 chips | +50 to +150 | +200 to +500 |
| Mean Reward | ~0.0 | +0.1 to +0.2 | +0.3 to +0.5 |

### Success Indicators âœ…

- **Win rate > 55%** - Successfully exploiting GTO
- **Positive mean payoff** - Winning chips consistently
- **Increasing bluff bonus** - Learning unconventional plays
- **High aggression rate** - Applying pressure effectively

### Warning Signs âš ï¸

- **Win rate < 45%** - Being counter-exploited
- **Negative mean payoff** - Losing money
- **No reward improvement** - Training not working

---

## ğŸ¯ Configuration Options

### Reward Profiles

**Ultra-Aggressive (Heavy Bluffing):**
```bash
python scripts/train_exploitative_llm.py \
  --gto-checkpoint checkpoints/checkpoint_1000.pt \
  --bluff-bonus 0.8 \
  --aggression-bonus 0.2 \
  --fold-equity-bonus 0.5 \
  --exploitative-sizing-bonus 0.3
```

**Balanced (Default):**
```bash
python scripts/train_exploitative_llm.py \
  --gto-checkpoint checkpoints/checkpoint_1000.pt \
  --bluff-bonus 0.5 \
  --aggression-bonus 0.1 \
  --fold-equity-bonus 0.3 \
  --exploitative-sizing-bonus 0.2
```

**Subtle (Unconventional Sizing Focus):**
```bash
python scripts/train_exploitative_llm.py \
  --gto-checkpoint checkpoints/checkpoint_1000.pt \
  --bluff-bonus 0.3 \
  --aggression-bonus 0.05 \
  --fold-equity-bonus 0.2 \
  --exploitative-sizing-bonus 0.5
```

---

## ğŸ’¡ Key Concepts

### How It Works

1. **GTO Opponent** - Loads your Deep CFR checkpoint and plays near-optimal poker
2. **LLM Agent** - Trained with LoRA to make decisions via text prompts
3. **Exploitative Rewards** - Bonuses for:
   - Bluffing with weak hands
   - Aggressive betting/raising
   - Making opponent fold
   - Unconventional bet sizing (overbets, tiny bets)

### Training Strategy

The model learns to **exploit GTO's predictability**:

- **GTO always folds at equilibrium frequency** â†’ LLM over-bluffs to exploit
- **GTO uses standard bet sizes** â†’ LLM uses unconventional sizes to confuse
- **GTO is balanced but predictable** â†’ LLM finds and exploits patterns

---

## ğŸ”§ Troubleshooting

### "No GTO checkpoint found"

```bash
# List your checkpoints
ls checkpoints/

# Make sure you have a trained GTO model
# If not, train one first:
modal run modal_deploy/train.py
```

### "TINKER_API_KEY not set"

```bash
# Set environment variable
export TINKER_API_KEY="sk-..."

# For Modal, create secret:
modal secret create tinker-api-key TINKER_API_KEY="sk-..."
```

### Training not improving

1. **Check GTO checkpoint quality** - Needs 500+ iterations
2. **Increase learning rate** - Try `--learning-rate 8e-5`
3. **More episodes** - Increase `--episodes-per-batch 32`
4. **Adjust bonuses** - Experiment with reward values

---

## ğŸ“ File Structure

```
pokerbot/
â”œâ”€â”€ llm_poker/
â”‚   â”œâ”€â”€ gto_opponent.py              # GTO agent (loads Deep CFR)
â”‚   â”œâ”€â”€ exploitative_env.py          # Environment with exploitative rewards
â”‚   â”œâ”€â”€ poker_env.py                 # Base environment
â”‚   â””â”€â”€ text_interface.py            # Text formatting
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_exploitative_llm.py    # Local training script
â”‚
â”œâ”€â”€ modal_deploy/
â”‚   â””â”€â”€ train_exploitative_llm.py    # Modal cloud deployment
â”‚
â”œâ”€â”€ EXPLOITATIVE_LLM_GUIDE.md        # Comprehensive guide
â””â”€â”€ QUICK_START_EXPLOITATIVE.md      # This file
```

---

## ğŸ® Next Steps

1. âœ… Run quick 10-batch test locally
2. âœ… Verify win rate > 50% and positive payoff
3. âœ… Launch full 100-200 batch training on Modal
4. âœ… Evaluate against GTO checkpoint
5. âœ… Iterate on reward bonuses based on results

---

## ğŸ’° Estimated Costs

| Training Type | Duration | Modal Cost | Tinker API Cost* |
|---------------|----------|------------|------------------|
| Quick test (10 batches) | ~30 min | ~$0.50 | ~$1-2 |
| Medium (100 batches) | ~3-4 hours | ~$3-5 | ~$10-15 |
| Full (200 batches) | ~8-12 hours | ~$8-12 | ~$20-30 |

*Tinker costs vary based on usage and model

---

## ğŸ“š Additional Resources

- **Full Guide:** `EXPLOITATIVE_LLM_GUIDE.md`
- **GTO Training:** `modal_deploy/train.py`
- **Environment Details:** `llm_poker/exploitative_env.py`
- **Tinker Docs:** [tinker.anthropic.com/docs](https://tinker.anthropic.com/docs)

---

## ğŸ†˜ Need Help?

1. Check you have a trained GTO checkpoint: `ls checkpoints/`
2. Verify Tinker API key: `echo $TINKER_API_KEY`
3. Start with smallest test: `--num-batches 5 --episodes-per-batch 4`
4. Review logs: `/tmp/tinker-examples/rl_poker_exploitative/`

Happy exploiting! ğŸƒğŸ’ª
