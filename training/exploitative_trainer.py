"""Exploitative Deep CFR training against a fixed GTO opponent.

This uses the SAME training method as the GTO model (Deep CFR),
but trains against a fixed GTO opponent instead of self-play.

This creates a "best response" strategy that exploits the GTO opponent.
"""

import torch
import numpy as np
from typing import Dict, List, Tuple
from collections import defaultdict

from poker_game.game import PokerGame, GameState, Action
from poker_game.state_encoder import StateEncoder
from poker_game.legacy_state_encoder import LegacyStateEncoder
from poker_game.information_set import get_information_set
from models.value_policy_net import ValuePolicyNet
from training.deep_cfr import DeepCFR


class ExploitativeTrainer:
    """Train a Deep CFR agent to exploit a fixed GTO opponent.

    Key difference from regular CFR:
    - Regular CFR: Both players update their strategies (converges to Nash)
    - Exploitative: Only our agent updates, opponent is fixed GTO (learns best response)

    This is also called "best response" or "exploitative training" in game theory.
    """

    def __init__(
        self,
        game: PokerGame,
        state_encoder: StateEncoder,
        gto_checkpoint_path: str,
        learning_rate: float = 5e-5,
        value_learning_rate: float = 5e-5,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
        use_legacy_encoder_for_gto: bool = True,
    ):
        """Initialize exploitative trainer.

        Args:
            game: PokerGame instance
            state_encoder: StateEncoder instance for exploitative agent
            gto_checkpoint_path: Path to trained GTO checkpoint
            learning_rate: Learning rate for policy network
            value_learning_rate: Learning rate for value network
            device: Device to run on ('cuda' or 'cpu')
            use_legacy_encoder_for_gto: If True, use LegacyStateEncoder for GTO
                                        (needed for checkpoint_iter_19.pt and earlier)
        """
        self.game = game
        self.state_encoder = state_encoder
        self.device = device

        # Initialize exploitative agent (the one we're training)
        input_dim = state_encoder.feature_dim
        exploitative_value_net = ValuePolicyNet(input_dim=input_dim).to(device)
        exploitative_policy_net = ValuePolicyNet(input_dim=input_dim).to(device)

        self.exploitative_agent = DeepCFR(
            value_net=exploitative_value_net,
            policy_net=exploitative_policy_net,
            state_encoder=state_encoder,
            game=game,
            learning_rate=learning_rate,
            value_learning_rate=value_learning_rate,
            device=device,
        )

        # Initialize GTO opponent (fixed, doesn't update)
        # Use legacy encoder if loading old checkpoints
        if use_legacy_encoder_for_gto:
            print("Using LegacyStateEncoder for GTO opponent (178 features)")
            gto_encoder = LegacyStateEncoder()
        else:
            print("Using current StateEncoder for GTO opponent (167 features)")
            gto_encoder = state_encoder

        self.gto_encoder = gto_encoder
        gto_input_dim = gto_encoder.feature_dim

        gto_value_net = ValuePolicyNet(input_dim=gto_input_dim).to(device)
        gto_policy_net = ValuePolicyNet(input_dim=gto_input_dim).to(device)

        self.gto_opponent = DeepCFR(
            value_net=gto_value_net,
            policy_net=gto_policy_net,
            state_encoder=gto_encoder,
            game=game,
            device=device,
        )

        # Load GTO checkpoint
        self._load_gto_checkpoint(gto_checkpoint_path)

        print(f"Initialized exploitative trainer vs GTO checkpoint: {gto_checkpoint_path}")
        print(f"  Exploitative agent: {input_dim} features")
        print(f"  GTO opponent: {gto_input_dim} features")
        print(f"  Device: {device}")

    def _load_gto_checkpoint(self, checkpoint_path: str):
        """Load GTO opponent checkpoint with architecture compatibility."""
        import os

        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"GTO checkpoint not found: {checkpoint_path}")

        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        print(f"Loading GTO checkpoint from: {checkpoint_path}")

        # Load networks with strict=False to allow architecture mismatches
        if 'value_net_state' in checkpoint:
            try:
                # Try strict loading first
                self.gto_opponent.value_net.load_state_dict(checkpoint['value_net_state'], strict=True)
                print("  ✓ Value network loaded (strict)")
            except RuntimeError as e:
                # Fall back to non-strict loading
                print(f"  ⚠ Value network architecture mismatch, loading with strict=False")
                missing, unexpected = self.gto_opponent.value_net.load_state_dict(
                    checkpoint['value_net_state'], strict=False
                )
                if missing:
                    print(f"    Missing keys: {missing}")
                if unexpected:
                    print(f"    Unexpected keys: {unexpected}")

        if 'policy_net_state' in checkpoint:
            try:
                # Try strict loading first
                self.gto_opponent.policy_net.load_state_dict(checkpoint['policy_net_state'], strict=True)
                print("  ✓ Policy network loaded (strict)")
            except RuntimeError as e:
                # Fall back to non-strict loading
                print(f"  ⚠ Policy network architecture mismatch, loading with strict=False")
                missing, unexpected = self.gto_opponent.policy_net.load_state_dict(
                    checkpoint['policy_net_state'], strict=False
                )
                if missing:
                    print(f"    Missing keys: {missing}")
                if unexpected:
                    print(f"    Unexpected keys: {unexpected}")

        # Load memories
        if 'regret_memory' in checkpoint:
            if isinstance(checkpoint['regret_memory'], dict):
                self.gto_opponent.regret_memory = defaultdict(
                    lambda: defaultdict(float),
                    {
                        k: defaultdict(float, v) if isinstance(v, dict) else v
                        for k, v in checkpoint['regret_memory'].items()
                    }
                )
                print(f"  ✓ Loaded {len(self.gto_opponent.regret_memory)} regret memory entries")
            else:
                self.gto_opponent.regret_memory = checkpoint['regret_memory']

        if 'strategy_memory' in checkpoint:
            if isinstance(checkpoint['strategy_memory'], dict):
                self.gto_opponent.strategy_memory = defaultdict(
                    lambda: defaultdict(float),
                    {
                        k: defaultdict(float, v) if isinstance(v, dict) else v
                        for k, v in checkpoint['strategy_memory'].items()
                    }
                )
                print(f"  ✓ Loaded {len(self.gto_opponent.strategy_memory)} strategy memory entries")
            else:
                self.gto_opponent.strategy_memory = checkpoint['strategy_memory']

        print(f"✓ Successfully loaded GTO checkpoint")

    def generate_exploitative_trajectories(self, num_trajectories: int) -> List[Dict]:
        """Generate trajectories where exploitative agent plays vs GTO opponent.

        Args:
            num_trajectories: Number of hands to play

        Returns:
            List of trajectory dictionaries
        """
        trajectories = []

        for _ in range(num_trajectories):
            # Start new hand
            state = self.game.reset()
            states = []
            info_sets = []

            # Play hand
            while not state.is_terminal:
                current_player = state.current_player
                legal_actions = self.game.get_legal_actions(state)

                if not legal_actions:
                    break

                # Get information set
                info_set = get_information_set(state, current_player)

                # Choose action based on which player
                if current_player == 0:
                    # Exploitative agent (training) - use current strategy
                    strategy = self.exploitative_agent.get_action_strategy(info_set, legal_actions)
                else:
                    # GTO opponent (fixed) - use average strategy (Nash equilibrium)
                    strategy = self.gto_opponent.get_average_strategy(info_set, legal_actions)

                if not strategy:
                    # Fallback to random
                    import random
                    action, amount = random.choice(legal_actions)
                else:
                    # Sample from strategy
                    actions = list(strategy.keys())
                    probs = np.array([strategy[a] for a in actions])
                    if probs.sum() > 0:
                        probs = probs / probs.sum()
                    else:
                        probs = np.ones(len(probs)) / len(probs)
                    chosen_idx = np.random.choice(len(actions), p=probs)
                    action, amount = actions[chosen_idx]

                # Store state info for player 0 (exploitative agent)
                if current_player == 0:
                    states.append(state)
                    info_sets.append(info_set)

                # Apply action
                state = self.game.apply_action(state, action, amount)

            # Get payoffs
            payoffs = self.game.get_payoff(state)

            # Create trajectory for player 0 (exploitative agent)
            trajectory = {
                'states': states,
                'info_sets': info_sets,
                'payoffs': payoffs,
                'player': 0,
            }

            trajectories.append(trajectory)

        return trajectories

    def train_iteration(self, num_trajectories: int, batch_size: int = 32) -> Dict:
        """Perform one training iteration.

        Args:
            num_trajectories: Number of trajectories to generate
            batch_size: Batch size for network training

        Returns:
            Dictionary of metrics
        """
        # Generate trajectories vs GTO
        trajectories = self.generate_exploitative_trajectories(num_trajectories)

        # Update exploitative agent using Deep CFR
        # This computes counterfactual values and updates networks
        value_buffer = []
        policy_buffer = []

        for trajectory in trajectories:
            payoff = trajectory['payoffs'][0]  # Player 0's payoff

            for state, info_set in zip(trajectory['states'], trajectory['info_sets']):
                legal_actions = self.game.get_legal_actions(state)

                # Encode state
                state_features = self.state_encoder.encode(state, 0)

                # Compute counterfactual values
                with torch.no_grad():
                    state_tensor = torch.FloatTensor(state_features).unsqueeze(0).to(self.device)
                    # value_net returns (value, policy_logits), we only need value
                    predicted_value, _ = self.exploitative_agent.value_net(state_tensor)
                    predicted_value = predicted_value.item()

                # Use payoff as target (simplified)
                target_value = payoff

                # Add to value buffer
                value_buffer.append({
                    'state': state_features,
                    'target': target_value,
                })

                # Get current strategy for this info set (index-based)
                strategy = self.exploitative_agent.get_strategy(info_set, legal_actions)

                # Compute regrets (simplified: positive if won, negative if lost)
                for action_idx in range(len(legal_actions)):
                    action_value = payoff  # Simplified
                    regret = action_value - predicted_value

                    # Update regret memory (using index, not action tuple)
                    self.exploitative_agent.regret_memory[info_set.key][action_idx] += regret

                # Add to policy buffer
                if strategy:
                    policy_buffer.append({
                        'state': state_features,
                        'strategy': strategy,
                        'legal_actions': legal_actions,
                    })

        # Train networks and track losses
        value_loss = 0.0
        policy_loss = 0.0

        if value_buffer:
            value_loss = self.exploitative_agent.train_value_network(value_buffer, batch_size)

        if policy_buffer:
            policy_loss = self.exploitative_agent.train_policy_network(policy_buffer, batch_size)

        # Calculate metrics
        avg_payoff = np.mean([t['payoffs'][0] for t in trajectories])
        wins = sum(1 for t in trajectories if t['payoffs'][0] > 0)
        win_rate = wins / len(trajectories) if trajectories else 0.0

        metrics = {
            'avg_payoff': avg_payoff,
            'win_rate': win_rate,
            'num_trajectories': len(trajectories),
            'value_buffer_size': len(value_buffer),
            'policy_buffer_size': len(policy_buffer),
            'value_loss': value_loss,
            'policy_loss': policy_loss,
        }

        return metrics

    def save_checkpoint(self, path: str, iteration: int, metrics: Dict):
        """Save exploitative agent checkpoint.

        Args:
            path: Path to save checkpoint
            iteration: Current iteration number
            metrics: Training metrics
        """
        checkpoint = {
            'iteration': iteration,
            'value_net_state': self.exploitative_agent.value_net.state_dict(),
            'policy_net_state': self.exploitative_agent.policy_net.state_dict(),
            'value_optimizer_state': self.exploitative_agent.value_optimizer.state_dict(),
            'policy_optimizer_state': self.exploitative_agent.policy_optimizer.state_dict(),
            'regret_memory': dict(self.exploitative_agent.regret_memory),
            'strategy_memory': dict(self.exploitative_agent.strategy_memory),
            'counterfactual_values': dict(self.exploitative_agent.counterfactual_values),
            'metrics': metrics,
        }

        torch.save(checkpoint, path)
        print(f"Saved checkpoint to {path}")

    def load_checkpoint(self, path: str) -> Tuple[int, Dict]:
        """Load exploitative agent checkpoint.

        Args:
            path: Path to checkpoint

        Returns:
            Tuple of (iteration, metrics)
        """
        checkpoint = torch.load(path, map_location=self.device, weights_only=False)

        self.exploitative_agent.value_net.load_state_dict(checkpoint['value_net_state'])
        self.exploitative_agent.policy_net.load_state_dict(checkpoint['policy_net_state'])

        if 'value_optimizer_state' in checkpoint:
            self.exploitative_agent.value_optimizer.load_state_dict(checkpoint['value_optimizer_state'])
        if 'policy_optimizer_state' in checkpoint:
            self.exploitative_agent.policy_optimizer.load_state_dict(checkpoint['policy_optimizer_state'])

        if 'regret_memory' in checkpoint:
            self.exploitative_agent.regret_memory = defaultdict(
                lambda: defaultdict(float),
                checkpoint['regret_memory']
            )
        if 'strategy_memory' in checkpoint:
            self.exploitative_agent.strategy_memory = defaultdict(
                lambda: defaultdict(float),
                checkpoint['strategy_memory']
            )
        if 'counterfactual_values' in checkpoint:
            self.exploitative_agent.counterfactual_values = defaultdict(
                float,
                checkpoint['counterfactual_values']
            )

        iteration = checkpoint.get('iteration', 0)
        metrics = checkpoint.get('metrics', {})

        print(f"Loaded checkpoint from iteration {iteration}")

        return iteration, metrics
